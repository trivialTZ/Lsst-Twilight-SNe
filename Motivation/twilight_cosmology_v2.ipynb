{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Cell 1 — Imports, paths, robust readers, QC + plotting helpers\n",
    "# =========================\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# optional ROOT reader (for FITRES ROOT)\n",
    "try:\n",
    "    import uproot  # type: ignore\n",
    "except Exception:\n",
    "    uproot = None\n",
    "\n",
    "# ---- user paths (HIGHZ + LOWZ) ----\n",
    "BASE = Path(\"/Users/tz/Documents/GitHub/Lsst-Twilight-SNe/Motivation/data_snana_EP_LSST\")\n",
    "\n",
    "# 1_SIM (HEAD/PHOT)\n",
    "HEAD_HIGHZ = BASE / \"1_SIM/PIP_EP-LSST_LSST_P21_HIGHZ/PIP_EP-LSST_LSST_P21_HIGHZ_SNIaMODEL00-0001_HEAD.FITS.gz\"\n",
    "PHOT_HIGHZ = BASE / \"1_SIM/PIP_EP-LSST_LSST_P21_HIGHZ/PIP_EP-LSST_LSST_P21_HIGHZ_SNIaMODEL00-0001_PHOT.FITS.gz\"\n",
    "\n",
    "HEAD_LOWZ  = BASE / \"1_SIM/PIP_EP-LSST_LSST_P21_LOWZ/PIP_EP-LSST_LSST_P21_LOWZ_SNIaMODEL00-0001_HEAD.FITS.gz\"\n",
    "PHOT_LOWZ  = BASE / \"1_SIM/PIP_EP-LSST_LSST_P21_LOWZ/PIP_EP-LSST_LSST_P21_LOWZ_SNIaMODEL00-0001_PHOT.FITS.gz\"\n",
    "\n",
    "# 2_LCFIT (dirs + canonical filenames)\n",
    "FITRES_HIGHZ_DIR = BASE / \"2_LCFIT/PIP_EP-LSST_LSST_P21_HIGHZ\"\n",
    "FITRES_LOWZ_DIR  = BASE / \"2_LCFIT/PIP_EP-LSST_LSST_P21_LOWZ\"\n",
    "FITRES_HIGHZ_ASCII = FITRES_HIGHZ_DIR / \"FITOPT000.FITRES.gz\"\n",
    "FITRES_LOWZ_ASCII  = FITRES_LOWZ_DIR  / \"FITOPT000.FITRES.gz\"   # try ascii first\n",
    "FITRES_LOWZ_ROOT   = FITRES_LOWZ_DIR  / \"FITOPT000.ROOT.gz\"     # fallback if uproot installed\n",
    "\n",
    "# ---- analysis controls ----\n",
    "Z_MAX = 1.20   # keep within this z for both detection and cosmology\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def _to_int64_safe(x):\n",
    "    if pd.isna(x):\n",
    "        return pd.NA\n",
    "    try:\n",
    "        if isinstance(x, bytes):\n",
    "            x = x.decode(errors=\"ignore\")\n",
    "        if isinstance(x, str):\n",
    "            x = x.strip()\n",
    "        return np.int64(int(float(x)))\n",
    "    except Exception:\n",
    "        return pd.NA\n",
    "\n",
    "def _clean_chars_inplace(df: pd.DataFrame) -> None:\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype.kind in (\"S\", \"O\", \"U\"):\n",
    "            try:\n",
    "                df[c] = df[c].astype(str).str.strip()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def _to_numeric_if_possible(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Try numeric conversion. If it raises, keep original.\"\"\"\n",
    "    try:\n",
    "        return pd.to_numeric(s)\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "def read_head_fits(head_path: Path) -> pd.DataFrame:\n",
    "    with fits.open(head_path) as hdul:\n",
    "        arr = np.array(hdul[1].data)\n",
    "    df = pd.DataFrame(arr.byteswap().newbyteorder())\n",
    "    _clean_chars_inplace(df)\n",
    "    # ID\n",
    "    if \"SNID\" in df.columns:\n",
    "        df[\"ID_int\"] = pd.Series([_to_int64_safe(v) for v in df[\"SNID\"]], dtype=\"Int64\")\n",
    "    else:\n",
    "        df[\"ID_int\"] = pd.Series([_to_int64_safe(v) for v in df.index], dtype=\"Int64\")\n",
    "    # z, mjd\n",
    "    zcol = \"REDSHIFT_FINAL\" if \"REDSHIFT_FINAL\" in df.columns else (\"REDSHIFT_TRUE\" if \"REDSHIFT_TRUE\" in df.columns else None)\n",
    "    if zcol is not None:\n",
    "        df[\"z\"] = pd.to_numeric(df[zcol], errors=\"coerce\")\n",
    "    if \"PEAKMJD\" in df.columns:\n",
    "        df[\"PEAKMJD\"] = pd.to_numeric(df[\"PEAKMJD\"], errors=\"coerce\")\n",
    "    # ensure integers for pointers if present\n",
    "    for c in (\"NOBS\",\"PTROBS_MIN\",\"PTROBS_MAX\"):\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "def read_fitres_ascii_gz(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Parse SNANA FITRES ASCII (.FITRES.gz) robustly.\"\"\"\n",
    "    with gzip.open(path, \"rt\") as f:\n",
    "        lines = f.readlines()\n",
    "    # find VARNAMES\n",
    "    names = None\n",
    "    start = 0\n",
    "    for i, L in enumerate(lines):\n",
    "        if L.strip().upper().startswith(\"VARNAMES:\"):\n",
    "            names = L.strip().split()[1:]\n",
    "            start = i + 1\n",
    "            break\n",
    "    if names is None:\n",
    "        raise RuntimeError(f\"VARNAMES not found in {path}\")\n",
    "    rows = []\n",
    "    for L in lines[start:]:\n",
    "        s = L.strip()\n",
    "        if (not s) or s.startswith(\"#\") or s.upper().startswith((\"VARNAMES\",\"NVAR\",\"END\",\"VERSION\",\"SNANA\")):\n",
    "            continue\n",
    "        toks = s.split()\n",
    "        if toks and toks[0].endswith(\":\"):\n",
    "            toks = toks[1:]\n",
    "        if len(toks) < len(names):\n",
    "            continue\n",
    "        rows.append(toks[:len(names)])\n",
    "    df = pd.DataFrame(rows, columns=names)\n",
    "    for c in df.columns:\n",
    "        df[c] = _to_numeric_if_possible(df[c])\n",
    "    return df\n",
    "\n",
    "def read_fitres_root(path: Path) -> pd.DataFrame:\n",
    "    if uproot is None:\n",
    "        raise RuntimeError(\"uproot is not available to read ROOT FITRES.\")\n",
    "    with uproot.open(path) as f:\n",
    "        tree = None\n",
    "        for key in (\"FITRES\", \"FITOPT000\", \"FITRES/FITRES\"):\n",
    "            if key in f:\n",
    "                tree = f[key]\n",
    "                break\n",
    "        if tree is None:\n",
    "            for _, obj in f.items():\n",
    "                if hasattr(obj, \"arrays\"):\n",
    "                    tree = obj\n",
    "                    break\n",
    "        if tree is None:\n",
    "            raise RuntimeError(f\"No TTree found in {path}\")\n",
    "        df = tree.arrays(library=\"pd\")\n",
    "    _clean_chars_inplace(df)\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def read_fitres_any(*candidates: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Try ASCII FITRES first (preferred), then ROOT (if uproot available).\n",
    "    If a directory is passed, search it for *.FITRES.gz (then *.ROOT*).\n",
    "    If nothing is readable, return empty DataFrame (with warning-like print).\n",
    "    \"\"\"\n",
    "    paths: list[Path] = []\n",
    "    for p in candidates:\n",
    "        if p is None:\n",
    "            continue\n",
    "        p = Path(p)\n",
    "        if p.is_file():\n",
    "            paths.append(p)\n",
    "        elif p.is_dir():\n",
    "            paths += sorted(p.glob(\"*.FITRES.gz\"))\n",
    "            paths += sorted(p.glob(\"*.ROOT*\"))\n",
    "\n",
    "    # prefer ASCII\n",
    "    for p in paths:\n",
    "        if p.suffixes[-2:] == [\".FITRES\", \".gz\"] or p.name.upper().endswith(\".FITRES.GZ\"):\n",
    "            return read_fitres_ascii_gz(p)\n",
    "    # then ROOT (if possible)\n",
    "    for p in paths:\n",
    "        if (\".ROOT\" in p.name.upper()) and (uproot is not None):\n",
    "            return read_fitres_root(p)\n",
    "\n",
    "    print(f\"[WARN] No readable FITRES among: {[str(pp) for pp in candidates]}. Continuing with empty FITRES.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def standardize_fitres(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=[\"ID_int\",\"z\",\"PKMJD\"])\n",
    "    _clean_chars_inplace(df)\n",
    "    # ID\n",
    "    if \"CIDint\" in df.columns:\n",
    "        df[\"ID_int\"] = pd.to_numeric(df[\"CIDint\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    elif \"CID\" in df.columns:\n",
    "        df[\"ID_int\"] = pd.Series([_to_int64_safe(v) for v in df[\"CID\"]], dtype=\"Int64\")\n",
    "    elif \"SNID\" in df.columns:\n",
    "        df[\"ID_int\"] = pd.Series([_to_int64_safe(v) for v in df[\"SNID\"]], dtype=\"Int64\")\n",
    "    else:\n",
    "        df[\"ID_int\"] = pd.Series([_to_int64_safe(v) for v in df.index], dtype=\"Int64\")\n",
    "\n",
    "    # redshift\n",
    "    z = None\n",
    "    for zc in (\"zHD\",\"zCMB\",\"z\",\"ZCMB\",\"Z\"):\n",
    "        if zc in df.columns:\n",
    "            z = pd.to_numeric(df[zc], errors=\"coerce\")\n",
    "            break\n",
    "    df[\"z\"] = z\n",
    "\n",
    "    # peak mjd\n",
    "    pk = None\n",
    "    for pc in (\"PKMJD\",\"PKMJD_SALT2\",\"PKMJD_SNIa\"):\n",
    "        if pc in df.columns:\n",
    "            pk = pd.to_numeric(df[pc], errors=\"coerce\")\n",
    "            break\n",
    "    df[\"PKMJD\"] = pk\n",
    "    return df\n",
    "\n",
    "def densest_year_window(mjd: np.ndarray, width_days: float = 365.25) -> tuple[float,float,int]:\n",
    "    \"\"\"Return (t0, t1, max_count) for a sliding window with max inclusions.\"\"\"\n",
    "    mjd = np.sort(mjd[np.isfinite(mjd)])\n",
    "    if mjd.size == 0:\n",
    "        return (np.nan, np.nan, 0)\n",
    "    j0 = 0\n",
    "    best = (mjd[0], mjd[0] + width_days, 1)\n",
    "    for j1 in range(mjd.size):\n",
    "        while mjd[j1] - mjd[j0] > width_days:\n",
    "            j0 += 1\n",
    "        cnt = j1 - j0 + 1\n",
    "        if cnt > best[2]:\n",
    "            best = (mjd[j0], mjd[j0] + width_days, cnt)\n",
    "    return best\n",
    "\n",
    "# ---- PHOT reader, ±10d/SN>5 check, Rosselli-style QC, N(z) + plot ----\n",
    "def read_phot_fits(path: Path) -> pd.DataFrame:\n",
    "    with fits.open(path) as hdul:\n",
    "        arr = np.array(hdul[1].data)\n",
    "    df = pd.DataFrame(arr.byteswap().newbyteorder())\n",
    "    for c in (\"MJD\",\"FLUXCAL\",\"FLUXCALERR\",\"FLUX\",\"FLUXERR\",\"PHOTFLAG\"):\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# ---------- Saturation helpers ----------\n",
    "from astropy.io import fits\n",
    "\n",
    "def discover_sat_mask_from_headers(phot_path: Path) -> int | None:\n",
    "    \"\"\"Return PHOTFLAG_SATURATE mask found anywhere in the PHOT FITS headers, else None.\"\"\"\n",
    "    try:\n",
    "        with fits.open(phot_path) as hdus:\n",
    "            for h in hdus:\n",
    "                if \"PHOTFLAG_SATURATE\" in h.header:\n",
    "                    return int(h.header[\"PHOTFLAG_SATURATE\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def epoch_saturation_mask_df(phot_df: pd.DataFrame, phot_path: Path | None = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build an epoch-level saturation boolean mask for a PHOT DataFrame.\n",
    "    Priority:\n",
    "      1) Use PHOTFLAG & PHOTFLAG_SATURATE bit (from header) if available.\n",
    "      2) Fallback sentinel: |FLUXCAL| < 1e-6 and |FLUXCALERR| > 1e7.\n",
    "    \"\"\"\n",
    "    # Ensure numeric coercion\n",
    "    for c in (\"FLUXCAL\",\"FLUXCALERR\",\"FLUX\",\"FLUXERR\",\"PHOTFLAG\"):\n",
    "        if c in phot_df.columns:\n",
    "            phot_df[c] = pd.to_numeric(phot_df[c], errors=\"coerce\")\n",
    "\n",
    "    # Try PHOTFLAG bit\n",
    "    if (\"PHOTFLAG\" in phot_df.columns) and (phot_path is not None):\n",
    "        sat_mask = discover_sat_mask_from_headers(phot_path)\n",
    "        if sat_mask is not None:\n",
    "            return (phot_df[\"PHOTFLAG\"].fillna(0).astype(np.int64) & int(sat_mask)) > 0\n",
    "\n",
    "    # Fallback sentinel (robust across sims)\n",
    "    if {\"FLUXCAL\",\"FLUXCALERR\"}.issubset(phot_df.columns):\n",
    "        return (phot_df[\"FLUXCAL\"].abs() < 1e-6) & (phot_df[\"FLUXCALERR\"].abs() > 1e7)\n",
    "    # Last resort: try raw flux columns if present\n",
    "    if {\"FLUX\",\"FLUXERR\"}.issubset(phot_df.columns):\n",
    "        return (phot_df[\"FLUX\"].abs() < 1e-6) & (phot_df[\"FLUXERR\"].abs() > 1e7)\n",
    "    # If neither convention is present, assume no epochs are flagged\n",
    "    return np.zeros(len(phot_df), dtype=bool)\n",
    "\n",
    "def _slice_indices_from_pointers(pmin, pmax, n_rows: int) -> tuple[int,int]:\n",
    "    \"\"\"\n",
    "    Map SNANA HEAD pointers to a Python slice [a:b).\n",
    "    Tries 1-based inclusive first, then 0-based inclusive.\n",
    "    \"\"\"\n",
    "    pmin = int(pmin) if pd.notna(pmin) else 0\n",
    "    pmax = int(pmax) if pd.notna(pmax) else -1\n",
    "    a = max(0, pmin - 1); b = min(n_rows, pmax)\n",
    "    if b > a:\n",
    "        return a, b\n",
    "    a = max(0, pmin); b = min(n_rows, pmax + 1)\n",
    "    if b > a:\n",
    "        return a, b\n",
    "    return (0, 0)\n",
    "\n",
    "def nosat_mask_for_run(\n",
    "    fit_df: pd.DataFrame,\n",
    "    head_df: pd.DataFrame,\n",
    "    phot_df: pd.DataFrame,\n",
    "    phot_path: Path | None = None,\n",
    "    rest_window: tuple[float,float] | None = None,  # e.g., (-10.0, +10.0) to ignore saturation outside ±10d\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return a boolean Series aligned to fit_df.index:\n",
    "      True  -> SN passes the \"no saturation\" cut (i.e., has ZERO saturated epochs in the selected window)\n",
    "      False -> SN has ≥1 saturated epoch in the selected window\n",
    "    If rest_window is None, look over all epochs; otherwise limit to [tmin, tmax] in rest-frame days from PKMJD.\n",
    "    \"\"\"\n",
    "    ok = pd.Series(True, index=fit_df.index, dtype=bool)\n",
    "\n",
    "    if phot_df is None or phot_df.empty or head_df is None or head_df.empty:\n",
    "        # No information → don't veto anything\n",
    "        return ok\n",
    "\n",
    "    # Build epoch-level saturation mask once\n",
    "    sat_epoch = epoch_saturation_mask_df(phot_df, phot_path=phot_path)\n",
    "    n_rows = len(phot_df)\n",
    "\n",
    "    # Fast lookup for HEAD pointers by SN ID\n",
    "    need_cols = {\"ID_int\",\"PTROBS_MIN\",\"PTROBS_MAX\"}\n",
    "    if not need_cols.issubset(head_df.columns):\n",
    "        return ok\n",
    "\n",
    "    head_idx = (head_df[list(need_cols)]\n",
    "                .dropna()\n",
    "                .astype({\"PTROBS_MIN\":\"Int64\",\"PTROBS_MAX\":\"Int64\"})\n",
    "                .set_index(\"ID_int\"))\n",
    "\n",
    "    # Iterate SNe in this run\n",
    "    for i, row in fit_df.iterrows():\n",
    "        sid = row.get(\"ID_int\", pd.NA)\n",
    "        if pd.isna(sid) or sid not in head_idx.index:\n",
    "            continue\n",
    "\n",
    "        pmin = head_idx.at[sid, \"PTROBS_MIN\"]\n",
    "        pmax = head_idx.at[sid, \"PTROBS_MAX\"]\n",
    "        a, b = _slice_indices_from_pointers(pmin, pmax, n_rows)\n",
    "        if b <= a:\n",
    "            continue\n",
    "\n",
    "        # Optionally restrict to a rest-frame window around peak\n",
    "        if rest_window is not None and {\"MJD\"}.issubset(phot_df.columns):\n",
    "            z  = pd.to_numeric(row.get(\"z\", np.nan), errors=\"coerce\")\n",
    "            pk = pd.to_numeric(row.get(\"PKMJD\", np.nan), errors=\"coerce\")\n",
    "            if np.isfinite(z) and np.isfinite(pk):\n",
    "                t_rest = (phot_df.loc[a:b, \"MJD\"].to_numpy() - pk) / (1.0 + z)\n",
    "                mwin = (t_rest >= rest_window[0]) & (t_rest <= rest_window[1])\n",
    "                if np.any(sat_epoch[a:b][mwin]):\n",
    "                    ok.at[i] = False\n",
    "                continue  # done with this SN (windowed path)\n",
    "\n",
    "        # Otherwise: any saturated epoch anywhere in the LC slice\n",
    "        if np.any(sat_epoch[a:b]):\n",
    "            ok.at[i] = False\n",
    "\n",
    "    return ok\n",
    "\n",
    "def _sn_pm10_snr5_ok_for_run(fit_df: pd.DataFrame, head_df: pd.DataFrame, phot_df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    For a single run (matching HEAD<->PHOT pointers), return a boolean Series (indexed like fit_df)\n",
    "    that is True if the SN has >=3 points with S/N>5 within ±10 rest-frame days of PKMJD.\n",
    "    Uses HEAD PTROBS_MIN/MAX to slice the PHOT table.\n",
    "    \"\"\"\n",
    "    ok = pd.Series(False, index=fit_df.index, dtype=bool)  # label-aligned\n",
    "\n",
    "    if phot_df is None or phot_df.empty:\n",
    "        return ok\n",
    "    if {\"FLUXCAL\",\"FLUXCALERR\"}.issubset(phot_df.columns):\n",
    "        flux_col, err_col = \"FLUXCAL\", \"FLUXCALERR\"\n",
    "    elif {\"FLUX\",\"FLUXERR\"}.issubset(phot_df.columns):\n",
    "        flux_col, err_col = \"FLUX\", \"FLUXERR\"\n",
    "    else:\n",
    "        return ok\n",
    "\n",
    "    if not {\"ID_int\",\"PTROBS_MIN\",\"PTROBS_MAX\"}.issubset(head_df.columns):\n",
    "        return ok\n",
    "\n",
    "    head_idx = (head_df[[\"ID_int\",\"PTROBS_MIN\",\"PTROBS_MAX\"]]\n",
    "                .dropna()\n",
    "                .astype({\"PTROBS_MIN\":\"Int64\",\"PTROBS_MAX\":\"Int64\"})\n",
    "                .set_index(\"ID_int\"))\n",
    "\n",
    "    for i, row in fit_df.iterrows():  # i is label\n",
    "        sid = row.get(\"ID_int\", pd.NA)\n",
    "        if pd.isna(sid) or sid not in head_idx.index:\n",
    "            continue\n",
    "        pmin = head_idx.at[sid, \"PTROBS_MIN\"]\n",
    "        pmax = head_idx.at[sid, \"PTROBS_MAX\"]\n",
    "        if pd.isna(pmin) or pd.isna(pmax):\n",
    "            continue\n",
    "\n",
    "        lo = max(int(pmin) - 1, 0)  # 1-based inclusive -> 0-based slice\n",
    "        hi = min(int(pmax), len(phot_df))\n",
    "        if hi <= lo:\n",
    "            continue\n",
    "\n",
    "        sl = phot_df.iloc[lo:hi]\n",
    "        z  = row.get(\"z\", np.nan)\n",
    "        pk = row.get(\"PKMJD\", np.nan)\n",
    "        if not (np.isfinite(z) and np.isfinite(pk)) or sl.empty:\n",
    "            continue\n",
    "\n",
    "        snr = sl[flux_col].to_numpy() / np.maximum(sl[err_col].to_numpy(), 1e-9)\n",
    "        t_rest = (sl[\"MJD\"].to_numpy() - pk) / (1.0 + z)\n",
    "        m = (t_rest >= -10.0) & (t_rest <= 10.0) & np.isfinite(snr)\n",
    "        if np.count_nonzero(snr[m] > 5.0) >= 3:\n",
    "            ok.loc[i] = True\n",
    "    return ok\n",
    "\n",
    "def ross_qc_with_report(fit_all: pd.DataFrame,\n",
    "                        head_hi: pd.DataFrame, phot_hi: pd.DataFrame,\n",
    "                        head_lo: pd.DataFrame, phot_lo: pd.DataFrame,\n",
    "                        phot_hi_path: Path | None = None,\n",
    "                        phot_lo_path: Path | None = None,\n",
    "                        sat_window_rest: tuple[float,float] | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply Rosselli+ style cuts to fit_all (already merged with HEAD columns) **and** drop any SN\n",
    "    with saturated epochs (any filter). If sat_window_rest is provided (e.g., (-10,+10)),\n",
    "    only saturation within that rest-frame window causes a veto.\n",
    "    \"\"\"\n",
    "    N0 = len(fit_all)\n",
    "    def keep_and_print(mask, tag):\n",
    "        kept = int(mask.sum())\n",
    "        print(f\"[ROSS QC] {tag:<18} keep={kept:6d}/{N0:6d} ({kept/N0:5.1%})\")\n",
    "        return mask\n",
    "\n",
    "    m_all = pd.Series(True, index=fit_all.index)\n",
    "\n",
    "    # Standard Ross+ cuts\n",
    "    m_fitprob = fit_all[\"FITPROB\"].fillna(0) > 0.05 if \"FITPROB\" in fit_all.columns else pd.Series(True, index=fit_all.index)\n",
    "    keep_and_print(m_fitprob, \"FITPROB cut\"); m_all &= m_fitprob\n",
    "\n",
    "    m_x1 = np.abs(pd.to_numeric(fit_all.get(\"x1\", np.nan), errors=\"coerce\")) <= 3.0\n",
    "    keep_and_print(m_x1, \"x1 range\"); m_all &= m_x1\n",
    "\n",
    "    m_c  = np.abs(pd.to_numeric(fit_all.get(\"c\",  np.nan), errors=\"coerce\"))  <= 0.3\n",
    "    keep_and_print(m_c, \"c range\");  m_all &= m_c\n",
    "\n",
    "    m_pkmjd = pd.to_numeric(fit_all.get(\"PKMJDERR\", np.nan), errors=\"coerce\") <= 1.0\n",
    "    keep_and_print(m_pkmjd, \"PKMJDERR<=1d\"); m_all &= m_pkmjd\n",
    "\n",
    "    m_x1e = pd.to_numeric(fit_all.get(\"x1ERR\", np.nan), errors=\"coerce\") <= 1.0\n",
    "    keep_and_print(m_x1e, \"x1ERR<=1\"); m_all &= m_x1e\n",
    "\n",
    "    m_ce  = pd.to_numeric(fit_all.get(\"cERR\",  np.nan), errors=\"coerce\")  <= 0.05\n",
    "    keep_and_print(m_ce,  \"cERR<=0.05\"); m_all &= m_ce\n",
    "\n",
    "    # >=3 obs ±10d with S/N>5, per run (you already had this)\n",
    "    ids_hi = set(head_hi[\"ID_int\"].dropna().astype(\"Int64\"))\n",
    "    ids_lo = set(head_lo[\"ID_int\"].dropna().astype(\"Int64\"))\n",
    "    fit_hi = fit_all[fit_all[\"ID_int\"].isin(ids_hi)]\n",
    "    fit_lo = fit_all[fit_all[\"ID_int\"].isin(ids_lo)]\n",
    "\n",
    "    obs_ok_hi = _sn_pm10_snr5_ok_for_run(fit_hi, head_hi, phot_hi)\n",
    "    obs_ok_lo = _sn_pm10_snr5_ok_for_run(fit_lo, head_lo, phot_lo)\n",
    "    m_obs = pd.Series(False, index=fit_all.index)\n",
    "    m_obs.loc[fit_hi.index] = obs_ok_hi.values\n",
    "    m_obs.loc[fit_lo.index] = obs_ok_lo.values\n",
    "    keep_and_print(m_obs, \">=3 obs ±10d\"); m_all &= m_obs\n",
    "\n",
    "    # NEW: No-saturation cut (any epoch unless sat_window_rest is set)\n",
    "    nosat_hi = nosat_mask_for_run(fit_hi, head_hi, phot_hi, phot_path=phot_hi_path, rest_window=sat_window_rest)\n",
    "    nosat_lo = nosat_mask_for_run(fit_lo, head_lo, phot_lo, phot_path=phot_lo_path, rest_window=sat_window_rest)\n",
    "    m_nosat = pd.Series(False, index=fit_all.index)\n",
    "    m_nosat.loc[fit_hi.index] = nosat_hi.values\n",
    "    m_nosat.loc[fit_lo.index] = nosat_lo.values\n",
    "    keep_and_print(m_nosat, \"no saturation\"); m_all &= m_nosat\n",
    "\n",
    "    kept = int(m_all.sum())\n",
    "    print(f\"[ROSS QC] TOTAL          keep={kept:6d}/{N0:6d} ({kept/N0:5.1%})\")\n",
    "    return fit_all.loc[m_all].copy()\n",
    "\n",
    "def nz_hist(z, z_edges):\n",
    "    h, _ = np.histogram(z, bins=z_edges)\n",
    "    return h\n",
    "\n",
    "def show_fig_y1_nz(z_mid, N_det, N_cos, N_cos_tw, DZ, Z_TW_MIN, Z_TW_MAX, Z_MAX):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(z_mid, N_det, label=f\"Detection (Y1, N={int(N_det.sum())})\")\n",
    "    plt.plot(z_mid, N_cos, label=f\"Cosmology (Y1, N={int(N_cos.sum())})\")\n",
    "    plt.plot(z_mid, N_cos_tw, label=f\"Cosmo + Twilight (Y1, N={int(N_cos_tw.sum())})\")\n",
    "    plt.axvspan(Z_TW_MIN, Z_TW_MAX, color=\"k\", alpha=0.06, label=\"Twilight Promotion\")\n",
    "    plt.xlim(0.0, Z_MAX)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlabel(f\"Redshift z\"); plt.ylabel(\"Count per Δz\")\n",
    "    plt.grid(alpha=0.2); plt.legend(); plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "d2760405666fbd09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Cell 2 — Load HIGHZ+LOWZ HEAD/FITRES, standardize, combine, Year-1 slice\n",
    "# =========================\n",
    "\n",
    "# HEAD tables (detection catalog info)\n",
    "head_hi = read_head_fits(HEAD_HIGHZ)\n",
    "head_lo = read_head_fits(HEAD_LOWZ)\n",
    "head_all = pd.concat([head_lo, head_hi], ignore_index=True)\n",
    "\n",
    "# Type Ia & z-range\n",
    "if \"SNTYPE\" in head_all.columns:\n",
    "    head_all = head_all.loc[head_all[\"SNTYPE\"] == 1].copy()\n",
    "head_all[\"z\"] = pd.to_numeric(head_all.get(\"z\", np.nan), errors=\"coerce\")\n",
    "head_all = head_all.loc[(head_all[\"z\"] > 0) & (head_all[\"z\"] <= Z_MAX)].copy()\n",
    "\n",
    "# FITRES (cosmology-fit outputs)\n",
    "fit_hi_raw = read_fitres_any(FITRES_HIGHZ_ASCII, FITRES_HIGHZ_DIR)\n",
    "fit_lo_raw = read_fitres_any(FITRES_LOWZ_ASCII if FITRES_LOWZ_ASCII.exists() else FITRES_LOWZ_DIR,\n",
    "                              FITRES_LOWZ_ROOT if FITRES_LOWZ_ROOT.exists() else None)\n",
    "\n",
    "fit_hi = standardize_fitres(fit_hi_raw)\n",
    "fit_lo = standardize_fitres(fit_lo_raw)\n",
    "fit_all = pd.concat([fit_lo, fit_hi], ignore_index=True)\n",
    "\n",
    "fit_all[\"z\"] = pd.to_numeric(fit_all.get(\"z\", np.nan), errors=\"coerce\")\n",
    "fit_all = fit_all.loc[(fit_all[\"z\"] > 0) & (fit_all[\"z\"] <= Z_MAX)].copy()\n",
    "\n",
    "# Merge HEAD columns into FITRES (carry pointers for ±10d cut)\n",
    "cols_keep = [\"ID_int\",\"z\"] + [c for c in (\"PEAKMJD\",\"SNTYPE\",\"RA\",\"DEC\",\"NOBS\",\"PTROBS_MIN\",\"PTROBS_MAX\") if c in head_all.columns]\n",
    "fit_all = fit_all.merge(head_all.loc[:, cols_keep], on=\"ID_int\", how=\"inner\", suffixes=(\"\", \"_HEAD\"))\n",
    "\n",
    "# Prefer z from FITRES when present, else HEAD\n",
    "fit_all[\"z\"] = np.where(np.isfinite(fit_all[\"z\"].values),\n",
    "                        fit_all[\"z\"].values,\n",
    "                        pd.to_numeric(fit_all.get(\"z_HEAD\", np.nan), errors=\"coerce\"))\n",
    "\n",
    "# Year-1 densest window by PKMJD (from FITRES)\n",
    "if \"PKMJD\" not in fit_all.columns or fit_all[\"PKMJD\"].notna().sum() == 0:\n",
    "    raise SystemExit(\"PKMJD not found in FITRES — required for Year-1 densest-window selection.\")\n",
    "t0, t1, _ = densest_year_window(fit_all[\"PKMJD\"].to_numpy(), 365.25)\n",
    "\n",
    "# Slice Y1: detection (HEAD by PEAKMJD) and cosmology (FITRES by PKMJD)\n",
    "if \"PEAKMJD\" in head_all.columns:\n",
    "    head_y1 = head_all.loc[(head_all[\"PEAKMJD\"] >= t0) & (head_all[\"PEAKMJD\"] < t1)].copy()\n",
    "else:\n",
    "    head_y1 = head_all.merge(fit_all[[\"ID_int\",\"PKMJD\"]], on=\"ID_int\", how=\"left\")\n",
    "    head_y1 = head_y1.loc[(head_y1[\"PKMJD\"] >= t0) & (head_y1[\"PKMJD\"] < t1)].copy()\n",
    "\n",
    "fitres_y1 = fit_all.loc[(fit_all[\"PKMJD\"] >= t0) & (fit_all[\"PKMJD\"] < t1)].copy()\n",
    "\n",
    "# Basic sanity prints\n",
    "def _z_stats(z):\n",
    "    z = np.asarray(z[np.isfinite(z)])\n",
    "    if z.size == 0:\n",
    "        return np.array([np.nan]*5)\n",
    "    return np.array([np.min(z), np.percentile(z,1), np.percentile(z,50),\n",
    "                     np.percentile(z,99), np.max(z)])\n",
    "\n",
    "zs_head = _z_stats(head_all[\"z\"])\n",
    "zs_fit  = _z_stats(fit_all[\"z\"])\n",
    "print(f\"[HEAD]   N={len(head_all):,}  z[min, p1, p50, p99, max] = [{zs_head[0]:.5f} {zs_head[1]:.5f} {zs_head[2]:.5f} {zs_head[3]:.5f} {zs_head[4]:.5f}]\")\n",
    "print(f\"[FITRES] N={len(fit_all):,}  z[min, p1, p50, p99, max] = [{zs_fit[0]:.5f} {zs_fit[1]:.5f} {zs_fit[2]:.5f} {zs_fit[3]:.5f} {zs_fit[4]:.5f}]\")\n",
    "print(f\"[Y1] window = [{t0:.1f}, {t1:.1f})  HEAD_Y1={len(head_y1):,}  FITRES_Y1={len(fitres_y1):,}\")\n"
   ],
   "id": "ad369a1cfebe1711",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Cell 3 — PHOT load, Ross QC, Y1 hist/figure, banded counts, WRITE binned CSVs\n",
    "# =========================\n",
    "\n",
    "# Load PHOT once (per run) for the ±10d cut\n",
    "phot_hi_df = read_phot_fits(PHOT_HIGHZ)\n",
    "phot_lo_df = read_phot_fits(PHOT_LOWZ)\n",
    "\n",
    "# Rosselli-style QC on the FULL fit catalog (before Y1 slice)\n",
    "fit_qc = ross_qc_with_report(\n",
    "    fit_all,\n",
    "    head_hi=head_hi, phot_hi=phot_hi_df,\n",
    "    head_lo=head_lo, phot_lo=phot_lo_df\n",
    ")\n",
    "\n",
    "# Cosmology Y1 = QC + Y1 time slice\n",
    "fit_qc_y1 = fit_qc.loc[(fit_qc[\"PKMJD\"] >= t0) & (fit_qc[\"PKMJD\"] < t1)].copy()\n",
    "\n",
    "# Histograms\n",
    "DZ = 0.01\n",
    "Z_TW_MIN, Z_TW_MAX = 0.02, 0.22\n",
    "\n",
    "z_edges = np.arange(0.0, Z_MAX + DZ + 1e-12, DZ)\n",
    "z_mid   = 0.5*(z_edges[:-1] + z_edges[1:])\n",
    "\n",
    "N_det = nz_hist(head_y1[\"z\"].to_numpy(float), z_edges)\n",
    "N_cos = nz_hist(fit_qc_y1[\"z\"].to_numpy(float), z_edges)\n",
    "\n",
    "# Twilight promotion at low-z\n",
    "band = (z_edges[:-1] >= Z_TW_MIN) & (z_edges[1:] <= Z_TW_MAX)\n",
    "N_cos_tw = N_cos.copy()\n",
    "N_cos_tw[band] = np.maximum(N_cos_tw[band], N_det[band])\n",
    "\n",
    "# Plot (figure)\n",
    "show_fig_y1_nz(z_mid, N_det, N_cos, N_cos_tw, DZ, Z_TW_MIN, Z_TW_MAX, Z_MAX)\n",
    "\n",
    "# Banded summary counts\n",
    "low_band = (z_mid >= Z_TW_MIN) & (z_mid < Z_TW_MAX)\n",
    "hi_band  = (z_mid >= Z_TW_MAX) & (z_mid < Z_MAX)\n",
    "\n",
    "det_low = int(N_det[low_band].sum()); det_hi = int(N_det[hi_band].sum())\n",
    "cos_low = int(N_cos[low_band].sum()); cos_hi = int(N_cos[hi_band].sum())\n",
    "tw_low  = int(N_cos_tw[low_band].sum()); tw_hi = int(N_cos_tw[hi_band].sum())\n",
    "\n",
    "print(\"\\n--- Summary Counts ---\")\n",
    "print(f\"[LOW-Z 0.02–0.14]  DET={det_low}  COS={cos_low}  COS_Tw={tw_low}\")\n",
    "print(f\"[HI-Z 0.14–1.20]   DET={det_hi}  COS={cos_hi}  COS_Tw={tw_hi}\")\n",
    "\n",
    "# --- Build sigma_mu per bin from the QC Year-1 cosmology sample and WRITE CSVs ---\n",
    "\n",
    "DERIVED = Path(\"/Users/tz/Documents/GitHub/Lsst-Twilight-SNe/Motivation/derived\")\n",
    "DERIVED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Per-SN distance-modulus uncertainty (robust to missing columns)\n",
    "def _sigma_mu_per_sn(df: pd.DataFrame) -> pd.Series:\n",
    "    alpha = pd.to_numeric(df.get(\"SIM_alpha\", df.get(\"alpha\", 0.14)), errors=\"coerce\").fillna(0.14)\n",
    "    beta  = pd.to_numeric(df.get(\"SIM_beta\",  df.get(\"beta\",  3.10)), errors=\"coerce\").fillna(3.10)\n",
    "    mBERR = pd.to_numeric(df.get(\"mBERR\", np.nan), errors=\"coerce\").fillna(0.12)\n",
    "    x1ERR = pd.to_numeric(df.get(\"x1ERR\", np.nan), errors=\"coerce\").fillna(0.9)\n",
    "    cERR  = pd.to_numeric(df.get(\"cERR\",  np.nan), errors=\"coerce\").fillna(0.04)\n",
    "    cov   = pd.to_numeric(df.get(\"COV_x1_c\", df.get(\"COV_x1c\", 0.0)), errors=\"coerce\").fillna(0.0)\n",
    "    z     = pd.to_numeric(df.get(\"z\", np.nan), errors=\"coerce\").astype(float)\n",
    "\n",
    "    mu2 = (mBERR**2) + (alpha*x1ERR)**2 + (beta*cERR)**2 - 2.0*alpha*beta*cov\n",
    "    sig_lens = 0.055*z\n",
    "    sig_vpec = (5.0/np.log(10.0))*(300.0/(299792.458*np.maximum(z, 1e-3)))\n",
    "    mu2 = mu2 + (0.08**2) + (sig_lens**2) + (sig_vpec**2)\n",
    "    return np.sqrt(np.maximum(mu2, 0.0))\n",
    "\n",
    "# per-SN sigma_mu on the QC+Y1 cosmology sample\n",
    "fit_qc_y1 = fit_qc_y1.copy()\n",
    "fit_qc_y1[\"sigma_mu_sn\"] = _sigma_mu_per_sn(fit_qc_y1)\n",
    "\n",
    "# per-bin median sigma_mu\n",
    "sigma_bin = np.full_like(z_mid, np.nan, dtype=float)\n",
    "for k in range(len(z_mid)):\n",
    "    m = (fit_qc_y1[\"z\"] >= z_edges[k]) & (fit_qc_y1[\"z\"] < z_edges[k+1])\n",
    "    if m.any():\n",
    "        sigma_bin[k] = float(fit_qc_y1.loc[m, \"sigma_mu_sn\"].median())\n",
    "\n",
    "# fill empty bins smoothly, fallback to global median\n",
    "if np.isnan(sigma_bin).any():\n",
    "    s = pd.Series(sigma_bin)\n",
    "    s = s.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    global_med = float(np.nanmedian(fit_qc_y1[\"sigma_mu_sn\"])) if \"sigma_mu_sn\" in fit_qc_y1 else 0.12\n",
    "    s = s.fillna(global_med)\n",
    "    sigma_bin = s.to_numpy()\n",
    "\n",
    "# write Fisher-ready BINNED CSVs (preferred & compat names)\n",
    "df_base = pd.DataFrame({\"z\": z_mid, \"N\": N_cos,    \"sigma_mu\": sigma_bin})\n",
    "df_tw   = pd.DataFrame({\"z\": z_mid, \"N\": N_cos_tw, \"sigma_mu\": sigma_bin})\n",
    "\n",
    "df_base.to_csv(DERIVED / \"y1_cat_bin_base_ep_lsst.csv\", index=False)\n",
    "df_tw.to_csv  (DERIVED / \"y1_cat_bin_tw_ep_lsst.csv\",   index=False)\n",
    "\n",
    "df_base.to_csv(DERIVED / \"y1_cat_bin_base_fix.csv\", index=False)\n",
    "df_tw.to_csv  (DERIVED / \"y1_cat_bin_tw_fix.csv\",   index=False)\n",
    "\n",
    "# optional: combined histogram dump\n",
    "pd.DataFrame({\n",
    "    \"z_lo\": z_edges[:-1], \"z_hi\": z_edges[1:],\n",
    "    \"N_det\": N_det, \"N_cosmo_WFD\": N_cos, \"N_cosmo_WFD_Tw\": N_cos_tw,\n",
    "    \"sigma_mu_bin\": sigma_bin\n",
    "}).to_csv(DERIVED / \"y1_nz_hist_ep_lsst.csv\", index=False)\n",
    "\n",
    "print(f\"[WRITE] wrote binned Fisher inputs to: {DERIVED.name}/\")\n",
    "print(\"        y1_cat_bin_base_ep_lsst.csv / y1_cat_bin_tw_ep_lsst.csv\")\n",
    "print(\"        y1_cat_bin_base_fix.csv  / y1_cat_bin_tw_fix.csv (compat)\")\n"
   ],
   "id": "a3a6993776132cae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Cell 3b — Create & write binned Fisher CSVs (run BEFORE Cell 4)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- config / paths ----\n",
    "DERIVED = Path(\"/Users/tz/Documents/GitHub/Lsst-Twilight-SNe/Motivation/derived\")\n",
    "DERIVED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DZ    = globals().get(\"DZ\", 0.01)\n",
    "Z_MAX = float(globals().get(\"Z_MAX\", 1.20))\n",
    "\n",
    "# ---- ensure we have Y1 samples ----\n",
    "# Expect head_y1 (detection, from HEAD) and fit_qc_y1 (QC+Y1 cosmology) from earlier cells.\n",
    "# Fallbacks are provided to make this cell self-contained.\n",
    "\n",
    "def _require_y1_samples():\n",
    "    global head_y1, fit_qc_y1, fitres_y1, t0, t1\n",
    "    # head_y1\n",
    "    if \"head_y1\" not in globals():\n",
    "        if \"head_all\" in globals() and \"fit_all\" in globals():\n",
    "            if \"t0\" not in globals() or \"t1\" not in globals():\n",
    "                t0, t1, _ = densest_year_window(fit_all[\"PKMJD\"].to_numpy(), 365.25)\n",
    "            if \"PEAKMJD\" in head_all.columns:\n",
    "                head_y1 = head_all.loc[(head_all[\"PEAKMJD\"] >= t0) & (head_all[\"PEAKMJD\"] < t1)].copy()\n",
    "            else:\n",
    "                head_y1 = head_all.merge(fit_all[[\"ID_int\",\"PKMJD\"]], on=\"ID_int\", how=\"left\")\n",
    "                head_y1 = head_y1.loc[(head_y1[\"PKMJD\"] >= t0) & (head_y1[\"PKMJD\"] < t1)].copy()\n",
    "        else:\n",
    "            raise RuntimeError(\"head_y1 is missing and head_all/fit_all not found. Run Cell 2 first.\")\n",
    "    # fit_qc_y1\n",
    "    if \"fit_qc_y1\" not in globals():\n",
    "        if \"fitres_y1\" in globals():\n",
    "            # fallback: no QC, use fitres_y1 directly\n",
    "            print(\"[WARN] fit_qc_y1 not found — using fitres_y1 (no QC) to build binned files.\")\n",
    "            fit_qc_y1 = fitres_y1.copy()\n",
    "        else:\n",
    "            raise RuntimeError(\"fit_qc_y1/fitres_y1 missing. Run QC/Cell 3 first.\")\n",
    "\n",
    "_require_y1_samples()\n",
    "\n",
    "# ---- build z-grid and histograms ----\n",
    "z_edges = np.arange(0.0, Z_MAX + DZ + 1e-12, DZ)\n",
    "z_mid   = 0.5*(z_edges[:-1] + z_edges[1:])\n",
    "\n",
    "def _nz(zvals):\n",
    "    return np.histogram(np.asarray(zvals, dtype=float), bins=z_edges)[0]\n",
    "\n",
    "# detection & cosmology counts\n",
    "N_det = _nz(head_y1[\"z\"])\n",
    "N_cos = _nz(fit_qc_y1[\"z\"])\n",
    "\n",
    "# twilight promotion\n",
    "band = (z_edges[:-1] >= Z_TW_MIN) & (z_edges[1:] <= Z_TW_MAX)\n",
    "N_cos_tw = N_cos.copy()\n",
    "N_cos_tw[band] = np.maximum(N_cos_tw[band], N_det[band])\n",
    "\n",
    "# ---- per-SN sigma_mu and per-bin median sigma_mu ----\n",
    "def _sigma_mu_per_sn(df: pd.DataFrame) -> pd.Series:\n",
    "    alpha = pd.to_numeric(df.get(\"SIM_alpha\", df.get(\"alpha\", 0.14)), errors=\"coerce\").fillna(0.14)\n",
    "    beta  = pd.to_numeric(df.get(\"SIM_beta\",  df.get(\"beta\",  3.10)), errors=\"coerce\").fillna(3.10)\n",
    "    mBERR = pd.to_numeric(df.get(\"mBERR\", np.nan), errors=\"coerce\").fillna(0.12)\n",
    "    x1ERR = pd.to_numeric(df.get(\"x1ERR\", np.nan), errors=\"coerce\").fillna(0.9)\n",
    "    cERR  = pd.to_numeric(df.get(\"cERR\",  np.nan), errors=\"coerce\").fillna(0.04)\n",
    "    cov   = pd.to_numeric(df.get(\"COV_x1_c\", df.get(\"COV_x1c\", 0.0)), errors=\"coerce\").fillna(0.0)\n",
    "    z     = pd.to_numeric(df.get(\"z\", np.nan), errors=\"coerce\").astype(float)\n",
    "    mu2 = (mBERR**2) + (alpha*x1ERR)**2 + (beta*cERR)**2 - 2.0*alpha*beta*cov\n",
    "    sig_lens = 0.055*z\n",
    "    sig_vpec = (5.0/np.log(10.0))*(300.0/(299792.458*np.maximum(z, 1e-3)))\n",
    "    mu2 = mu2 + (0.08**2) + (sig_lens**2) + (sig_vpec**2)\n",
    "    return np.sqrt(np.maximum(mu2, 0.0))\n",
    "\n",
    "fit_qc_y1 = fit_qc_y1.copy()\n",
    "fit_qc_y1[\"sigma_mu_sn\"] = _sigma_mu_per_sn(fit_qc_y1)\n",
    "\n",
    "sigma_bin = np.full_like(z_mid, np.nan, dtype=float)\n",
    "for k in range(len(z_mid)):\n",
    "    m = (fit_qc_y1[\"z\"] >= z_edges[k]) & (fit_qc_y1[\"z\"] < z_edges[k+1])\n",
    "    if m.any():\n",
    "        sigma_bin[k] = float(fit_qc_y1.loc[m, \"sigma_mu_sn\"].median())\n",
    "\n",
    "# fill missing bins by interpolation then fallback to global median\n",
    "if np.isnan(sigma_bin).any():\n",
    "    s = pd.Series(sigma_bin).fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    global_med = float(np.nanmedian(fit_qc_y1[\"sigma_mu_sn\"])) if \"sigma_mu_sn\" in fit_qc_y1 else 0.12\n",
    "    sigma_bin = s.fillna(global_med).to_numpy()\n",
    "\n",
    "# ---- write Fisher-ready binned CSVs (both preferred + compat names) ----\n",
    "df_base = pd.DataFrame({\"z\": z_mid, \"N\": N_cos,    \"sigma_mu\": sigma_bin})\n",
    "df_tw   = pd.DataFrame({\"z\": z_mid, \"N\": N_cos_tw, \"sigma_mu\": sigma_bin})\n",
    "\n",
    "base_ep = DERIVED / \"y1_cat_bin_base_ep_lsst.csv\"\n",
    "tw_ep   = DERIVED / \"y1_cat_bin_tw_ep_lsst.csv\"\n",
    "base_fx = DERIVED / \"y1_cat_bin_base_fix.csv\"\n",
    "tw_fx   = DERIVED / \"y1_cat_bin_tw_fix.csv\"\n",
    "\n",
    "df_base.to_csv(base_ep, index=False); df_tw.to_csv(tw_ep, index=False)\n",
    "df_base.to_csv(base_fx, index=False); df_tw.to_csv(tw_fx, index=False)\n",
    "\n",
    "# Your new scenario: double counts in the twilight band\n",
    "df_tw_guess = df_tw.copy()\n",
    "df_tw_guess.loc[band, 'N'] = (df_base.loc[band, 'N'] * 2).astype(int)\n",
    "tw_guess_ep = DERIVED / \"y1_cat_bin_tw_guess_ep_lsst.csv\"\n",
    "df_tw_guess.to_csv(tw_guess_ep, index=False)\n",
    "df_tw_guess.to_csv(DERIVED / \"y1_cat_bin_tw_guess_fix.csv\", index=False) # Optional: add a compat name for the guess\n",
    "\n",
    "\n",
    "# optional: combined histogram dump (useful for QA)\n",
    "pd.DataFrame({\n",
    "    \"z_lo\": z_edges[:-1], \"z_hi\": z_edges[1:],\n",
    "    \"N_det\": N_det, \"N_cosmo_WFD\": N_cos, \"N_cosmo_WFD_Tw\": N_cos_tw,\n",
    "    \"N_cosmo_WFD_Tw_Guess\": df_tw_guess['N'], # Add your new scenario to the QA file\n",
    "    \"sigma_mu_bin\": sigma_bin\n",
    "}).to_csv(DERIVED / \"y1_nz_hist_ep_lsst.csv\", index=False)\n",
    "\n",
    "print(f\"[WRITE] Created binned files in {DERIVED}:\")\n",
    "print(\"        - y1_cat_bin_base_ep_lsst.csv\")\n",
    "print(\"        - y1_cat_bin_tw_ep_lsst.csv\")\n",
    "print(\"        - y1_cat_bin_tw_guess_ep_lsst.csv (Your Guess scenario)\") # Add a printout for confirmation\n",
    "print(\"        - y1_cat_bin_base_fix.csv  (compat)\")\n",
    "print(\"        - y1_cat_bin_tw_fix.csv    (compat)\")\n",
    "print(f\"        Totals -> WFD N={int(df_base['N'].sum())}, WFD+Twilight N={int(df_tw['N'].sum())}, WFD+Twilight (Guess) N={int(df_tw_guess['N'].sum())}\") # Update the final printout\n",
    "\n"
   ],
   "id": "795a84bd0cced0e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Cell 4 — Load binned catalogs (now that Cell 3 wrote them) + print dataset info\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "\n",
    "DERIVED = Path(\"/Users/tz/Documents/GitHub/Lsst-Twilight-SNe/Motivation/derived\")\n",
    "\n",
    "# Preferred filenames produced by Cell 3\n",
    "BASE_FILE = DERIVED / \"y1_cat_bin_base_ep_lsst.csv\"   # WFD-only cosmology (combined)\n",
    "TW_FILE   = DERIVED / \"y1_cat_bin_tw_ep_lsst.csv\"     # WFD+Twilight cosmology (combined)\n",
    "\n",
    "def _latest(pattern: str) -> Path:\n",
    "    paths = sorted(DERIVED.glob(pattern), key=lambda p: p.stat().st_mtime)\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No files match: {pattern}\")\n",
    "    return paths[-1]\n",
    "\n",
    "# Fallbacks if you kept a different suffix (compat names written in Cell 3)\n",
    "if not BASE_FILE.exists():\n",
    "    BASE_FILE = _latest(\"y1_cat_bin_base_*.csv\")\n",
    "if not TW_FILE.exists():\n",
    "    TW_FILE   = _latest(\"y1_cat_bin_tw_*.csv\")\n",
    "\n",
    "print(f\"[binned] using BASE: {BASE_FILE.name}\")\n",
    "print(f\"[binned] using TW  : {TW_FILE.name}\")\n",
    "\n",
    "df_wfd_bin    = pd.read_csv(BASE_FILE)\n",
    "df_twfull_bin = pd.read_csv(TW_FILE)\n",
    "\n",
    "\n",
    "# Load your new \"guess\" scenario CSV\n",
    "TW_GUESS_FILE = DERIVED / \"y1_cat_bin_tw_guess_ep_lsst.csv\"\n",
    "print(f\"[binned] using GUESS: {TW_GUESS_FILE.name}\")\n",
    "df_twguess_bin = pd.read_csv(TW_GUESS_FILE)\n",
    "\n",
    "# Also apply the Z_MAX cut to it\n",
    "if \"Z_MAX\" in globals():\n",
    "    df_twguess_bin = df_twguess_bin.query(\"0 < z <= @Z_MAX\").copy()\n",
    "\n",
    "\n",
    "# Safety checks and optional z-cut\n",
    "assert {\"z\",\"N\",\"sigma_mu\"}.issubset(df_wfd_bin.columns)\n",
    "assert {\"z\",\"N\",\"sigma_mu\"}.issubset(df_twfull_bin.columns)\n",
    "if \"Z_MAX\" in globals():\n",
    "    df_wfd_bin    = df_wfd_bin.query(\"0 < z <= @Z_MAX\").copy()\n",
    "    df_twfull_bin = df_twfull_bin.query(\"0 < z <= @Z_MAX\").copy()\n",
    "\n",
    "# Print the exact binned data fed into Fisher\n",
    "def _info_binned(name, df):\n",
    "    nz_mask = df[\"N\"] > 0\n",
    "    Ntot    = int(df[\"N\"].sum())\n",
    "    nbins   = int(nz_mask.sum())\n",
    "    if nbins:\n",
    "        zmin = float(df.loc[nz_mask, \"z\"].min())\n",
    "        zmax = float(df.loc[nz_mask, \"z\"].max())\n",
    "        zbar = float(np.average(df.loc[nz_mask, \"z\"], weights=df.loc[nz_mask, \"N\"]))\n",
    "        sigm = float(df.loc[nz_mask, \"sigma_mu\"].median())\n",
    "    else:\n",
    "        zmin = zmax = zbar = sigm = np.nan\n",
    "    wsum = float((df[\"N\"] / (df[\"sigma_mu\"]**2)).sum())\n",
    "    print(f\"[{name}] bins>0={nbins:4d}  N_total={Ntot:7,d}  \"\n",
    "          f\"z[min,⟨z⟩,max]=[{zmin:0.3f}, {zbar:0.3f}, {zmax:0.3f}]  \"\n",
    "          f\"median σμ={sigm:0.3f}  Σ(N/σμ²)={wsum:0.1f}\")\n",
    "\n",
    "_info_binned(\"WFD\", df_wfd_bin)\n",
    "_info_binned(\"WFD+Twilight\", df_twfull_bin)\n",
    "\n",
    "# (Optional) peek first/last few active bins\n",
    "def _head_tail_nonzero(df, k=3):\n",
    "    nz = df[df[\"N\"] > 0][[\"z\",\"N\",\"sigma_mu\"]]\n",
    "    print(\"  first bins:\\n\", nz.head(k).to_string(index=False))\n",
    "    print(\"  last  bins:\\n\", nz.tail(k).to_string(index=False))\n",
    "\n",
    "_head_tail_nonzero(df_wfd_bin)\n",
    "_head_tail_nonzero(df_twfull_bin)\n"
   ],
   "id": "14745c42b1bf4e4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "C_KMS = 299792.458  # km/s\n",
    "\n",
    "@dataclass\n",
    "class CosmoParams:\n",
    "    Om: float = 0.3\n",
    "    w0: float = -1.0\n",
    "    wa: float = 0.0\n",
    "    H0: float = 70.0\n",
    "    M: float = 0.0\n",
    "\n",
    "def Ez_flat_w0wa(z: np.ndarray, p: CosmoParams) -> np.ndarray:\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    Om = p.Om\n",
    "    Ode = 1.0 - Om\n",
    "    de_factor = (1.0 + z) ** (3.0 * (1.0 + p.w0 + p.wa)) * np.exp(-3.0 * p.wa * z / (1.0 + z))\n",
    "    return np.sqrt(Om * (1.0 + z) ** 3 + Ode * de_factor)\n",
    "\n",
    "def DC_Mpc(z: np.ndarray, p: CosmoParams, n_steps: int = 4096) -> np.ndarray:\n",
    "    z = np.atleast_1d(z).astype(float)\n",
    "    zmax = np.max(z)\n",
    "    zz = np.linspace(0.0, zmax, n_steps)\n",
    "    invE = 1.0 / Ez_flat_w0wa(zz, p)\n",
    "    if len(zz) % 2 == 0:\n",
    "        zz = zz[:-1]\n",
    "        invE = invE[:-1]\n",
    "    primitive = np.cumsum((invE[:-1] + invE[1:]) * 0.5 * (zz[1:] - zz[:-1]))\n",
    "    primitive = np.concatenate([[0.0], primitive])\n",
    "    integral = np.interp(z, zz, primitive)\n",
    "    return (C_KMS / p.H0) * integral\n",
    "\n",
    "def DL_Mpc(z: np.ndarray, p: CosmoParams) -> np.ndarray:\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return (1.0 + z) * DC_Mpc(z, p)\n",
    "\n",
    "def mu_theory(z: np.ndarray, p: CosmoParams) -> np.ndarray:\n",
    "    dl = DL_Mpc(z, p)\n",
    "    return 5.0 * np.log10(dl) + 25.0 + p.M\n",
    "\n",
    "@dataclass\n",
    "class FisherSetup:\n",
    "    vary_params: Tuple[str, ...]\n",
    "    fid: CosmoParams = field(default_factory=CosmoParams)\n",
    "    step_frac: Dict[str, float] = field(default_factory=lambda: {'Om': 1e-3, 'w0': 1e-3, 'wa': 1e-3, 'M': 1e-3})\n",
    "\n",
    "def jacobian_mu(z: np.ndarray, p: CosmoParams, setup: FisherSetup) -> np.ndarray:\n",
    "    J_cols = []\n",
    "    for name in setup.vary_params + ('M',):\n",
    "        if name == 'M':\n",
    "            J_cols.append(np.ones_like(z, dtype=float))\n",
    "            continue\n",
    "        step = setup.step_frac[name] * getattr(p, name if name != 'Om' else 'Om')\n",
    "        if step == 0.0:\n",
    "            step = 1e-5\n",
    "        p_plus = CosmoParams(**vars(p))\n",
    "        p_minus = CosmoParams(**vars(p))\n",
    "        setattr(p_plus, name, getattr(p_plus, name) + step)\n",
    "        setattr(p_minus, name, getattr(p_minus, name) - step)\n",
    "        mu_p = mu_theory(z, p_plus)\n",
    "        mu_m = mu_theory(z, p_minus)\n",
    "        J_cols.append((mu_p - mu_m) / (2.0 * step))\n",
    "    J = np.vstack(J_cols).T\n",
    "    return J\n",
    "\n",
    "def fisher_SN(z: np.ndarray, sigma_mu: np.ndarray, setup: FisherSetup) -> Tuple[np.ndarray, List[str]]:\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    sigma_mu = np.asarray(sigma_mu, dtype=float)\n",
    "    assert z.shape == sigma_mu.shape\n",
    "    p = setup.fid\n",
    "    J = jacobian_mu(z, p, setup)\n",
    "    Cinv = np.diag(1.0 / (sigma_mu ** 2))\n",
    "    F_full = J.T @ Cinv @ J\n",
    "    n = F_full.shape[0]\n",
    "    idx_M = n - 1\n",
    "    idx_theta = list(range(n - 1))\n",
    "    F_tt = F_full[np.ix_(idx_theta, idx_theta)]\n",
    "    F_tM = F_full[np.ix_(idx_theta, [idx_M])]\n",
    "    F_Mt = F_full[np.ix_([idx_M], idx_theta)]\n",
    "    F_MM = F_full[idx_M, idx_M]\n",
    "    F_marg = F_tt - F_tM @ np.linalg.inv(np.array([[F_MM]])) @ F_Mt\n",
    "    labels = list(setup.vary_params)\n",
    "    return F_marg, labels\n",
    "\n",
    "def cov_to_sigmas(F: np.ndarray, labels: List[str]) -> Tuple[Dict[str, float], np.ndarray]:\n",
    "    C = np.linalg.inv(F)\n",
    "    errs = {lab: float(np.sqrt(C[i, i])) for i, lab in enumerate(labels)}\n",
    "    return errs, C\n",
    "\n",
    "EXPECTED_COLS = ['z', 'sigma_mu']\n",
    "\n",
    "def load_catalog(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    missing = [c for c in EXPECTED_COLS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{path} is missing required columns: {missing}\")\n",
    "    return df[EXPECTED_COLS].copy()\n",
    "\n",
    "def combine_wfd_twilight(df_wfd: pd.DataFrame, df_twilight: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.concat([df_wfd, df_twilight], ignore_index=True)\n",
    "\n",
    "def sigma_mu_from_salt2_row(\n",
    "    mBERR: float,\n",
    "    x1ERR: float,\n",
    "    cERR: float,\n",
    "    COV_mB_x1: float,\n",
    "    COV_mB_c: float,\n",
    "    COV_x1_c: float,\n",
    "    z: float,\n",
    "    alpha: float = 0.14,\n",
    "    beta: float = 3.1,\n",
    "    sigma_int: float = 0.10,\n",
    "    sigma_vpec_kms: float = 300.0,\n",
    ") -> float:\n",
    "    var = (\n",
    "        mBERR**2\n",
    "        + (alpha**2) * (x1ERR**2)\n",
    "        + (beta**2) * (cERR**2)\n",
    "        + 2.0 * alpha * COV_mB_x1\n",
    "        - 2.0 * beta * COV_mB_c\n",
    "        - 2.0 * alpha * beta * COV_x1_c\n",
    "        + sigma_int**2\n",
    "    )\n",
    "    if z <= 0:\n",
    "        z = 1e-4\n",
    "    var += (5.0 / np.log(10.0)) ** 2 * (sigma_vpec_kms / (C_KMS * z)) ** 2\n",
    "    return float(np.sqrt(var))\n",
    "\n",
    "def build_catalog_from_fitres(\n",
    "    fitres_csv_path: str,\n",
    "    alpha: float = 0.14,\n",
    "    beta: float = 3.1,\n",
    "    sigma_int: float = 0.10,\n",
    "    sigma_vpec_kms: float = 300.0,\n",
    "    z_col: str = 'zHD',\n",
    ") -> pd.DataFrame:\n",
    "    df = pd.read_csv(fitres_csv_path)\n",
    "    needed = ['mBERR', 'x1ERR', 'cERR', 'COV_x1_c', 'COV_mB_c', 'COV_mB_x1', z_col]\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"FITRES is missing required columns: {missing}\")\n",
    "    sigmas = []\n",
    "    zs = df[z_col].values\n",
    "    for i in range(len(df)):\n",
    "        sig = sigma_mu_from_salt2_row(\n",
    "            mBERR=df.loc[i, 'mBERR'],\n",
    "            x1ERR=df.loc[i, 'x1ERR'],\n",
    "            cERR=df.loc[i, 'cERR'],\n",
    "            COV_mB_x1=df.loc[i, 'COV_mB_x1'],\n",
    "            COV_mB_c=df.loc[i, 'COV_mB_c'],\n",
    "            COV_x1_c=df.loc[i, 'COV_x1_c'],\n",
    "            z=zs[i],\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            sigma_int=sigma_int,\n",
    "            sigma_vpec_kms=sigma_vpec_kms,\n",
    "        )\n",
    "        sigmas.append(sig)\n",
    "    out = pd.DataFrame({'z': zs, 'sigma_mu': sigmas})\n",
    "    return out\n",
    "\n",
    "def run_forecast(df_wfd: pd.DataFrame, df_twilight: pd.DataFrame, model: str = 'lcdm') -> Dict[str, Dict]:\n",
    "    if model not in {'lcdm', 'w0wa'}:\n",
    "        raise ValueError(\"model must be 'lcdm' or 'w0wa'\")\n",
    "    if model == 'lcdm':\n",
    "        vary = ('Om',)\n",
    "    else:\n",
    "        vary = ('Om', 'w0', 'wa')\n",
    "    setup = FisherSetup(vary_params=vary, fid=CosmoParams())\n",
    "    df_combo = combine_wfd_twilight(df_wfd, df_twilight)\n",
    "    out = {}\n",
    "    for label, df in [('WFD', df_wfd), ('WFD+Twilight', df_combo)]:\n",
    "        F, labels = fisher_SN(df['z'].values, df['sigma_mu'].values, setup)\n",
    "        errs, C = cov_to_sigmas(F, labels)\n",
    "        out[label] = {'errs': errs, 'cov': C, 'labels': labels, 'N': len(df)}\n",
    "    return out\n",
    "\n",
    "def pretty_print_results(res: Dict[str, Dict]):\n",
    "    for key in res:\n",
    "        labels = res[key]['labels']\n",
    "        errs = res[key]['errs']\n",
    "        N = res[key]['N']\n",
    "        print(f\"\\n=== {key} (N={N}) ===\")\n",
    "        for lab in labels:\n",
    "            print(f\"σ({lab}) = {errs[lab]:.4f}\")\n",
    "\n",
    "def plot_pairwise_contours(res, pair=('w0','wa')):\n",
    "    fig = plt.figure(dpi=150)\n",
    "    ax = plt.gca()\n",
    "    xmin = ymin = +1e9\n",
    "    xmax = ymax = -1e9\n",
    "    centers = {'Om': 0.3, 'w0': -1.0, 'wa': 0.0}\n",
    "    for label in ['WFD', 'WFD+Twilight']:\n",
    "        labels = res[label]['labels']\n",
    "        C = res[label]['cov']\n",
    "        i = labels.index(pair[0])\n",
    "        j = labels.index(pair[1])\n",
    "        cov2 = C[np.ix_([i,j],[i,j])]\n",
    "        vals, vecs = np.linalg.eigh(cov2)\n",
    "        order = vals.argsort()[::-1]\n",
    "        vals = vals[order]\n",
    "        vecs = vecs[:, order]\n",
    "        angle = np.arctan2(vecs[1, 0], vecs[0, 0])\n",
    "        a = np.sqrt(vals[0]); b = np.sqrt(vals[1])\n",
    "        cx = centers.get(pair[0], 0.0)\n",
    "        cy = centers.get(pair[1], 0.0)\n",
    "        t = np.linspace(0, 2*np.pi, 200)\n",
    "        for nsig in (1.0, 2.0):\n",
    "            x = cx + nsig * (a*np.cos(t)*np.cos(angle) - b*np.sin(t)*np.sin(angle))\n",
    "            y = cy + nsig * (a*np.cos(t)*np.sin(angle) + b*np.sin(t)*np.cos(angle))\n",
    "            ax.plot(x, y, label=f'{label} {nsig:.0f}σ' if nsig==1.0 else None)\n",
    "            xmin, xmax = min(xmin, x.min()), max(xmax, x.max())\n",
    "            ymin, ymax = min(ymin, y.min()), max(ymax, y.max())\n",
    "    ax.set_xlabel(pair[0]); ax.set_ylabel(pair[1]); ax.legend()\n",
    "    padx = 0.1*(xmax-xmin); pady = 0.1*(ymax-ymin)\n",
    "    ax.set_xlim(xmin-padx, xmax+padx); ax.set_ylim(ymin-pady, ymax+pady)\n",
    "    plt.title(f'Forecast contours: {pair[0]} vs {pair[1]}')\n",
    "    plt.show()\n"
   ],
   "id": "14581c997f637adf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Cell 5 — Fisher: ΛCDM & w0waCDM plots, with dataset info already printed by Cell 4\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fisher from binned\n",
    "def fisher_from_binned(df_bin: pd.DataFrame, setup: FisherSetup):\n",
    "    z = df_bin[\"z\"].to_numpy(float)\n",
    "    w = (df_bin[\"N\"].to_numpy(float)) / (df_bin[\"sigma_mu\"].to_numpy(float) ** 2)\n",
    "    J = jacobian_mu(z, CosmoParams(), setup)  # last column is dμ/dM\n",
    "    W = np.diag(w)\n",
    "    F_full = J.T @ W @ J\n",
    "    # marginalize over nuisance M (last)\n",
    "    n = F_full.shape[0]\n",
    "    idx_M = n - 1\n",
    "    idx_theta = list(range(n - 1))\n",
    "    F_tt = F_full[np.ix_(idx_theta, idx_theta)]\n",
    "    F_tM = F_full[np.ix_(idx_theta, [idx_M])]\n",
    "    F_Mt = F_full[np.ix_([idx_M], idx_theta)]\n",
    "    F_MM = F_full[idx_M, idx_M]\n",
    "    F_marg = F_tt - F_tM @ np.linalg.inv(np.array([[F_MM]])) @ F_Mt\n",
    "    labels = list(setup.vary_params)\n",
    "    return F_marg, labels\n",
    "\n",
    "def run_binned_forecast(df_base, df_twfull, model=\"lcdm\"):\n",
    "    vary = (\"Om\",) if model==\"lcdm\" else (\"Om\",\"w0\",\"wa\")\n",
    "    setup = FisherSetup(vary_params=vary, fid=CosmoParams())\n",
    "    # WFD\n",
    "    F_wfd, labels = fisher_from_binned(df_base, setup)\n",
    "    errs_wfd, C_wfd = cov_to_sigmas(F_wfd, labels)\n",
    "    # WFD+Twilight\n",
    "    F_tw, _ = fisher_from_binned(df_twfull, setup)\n",
    "    errs_tw, C_tw = cov_to_sigmas(F_tw, labels)\n",
    "    return {\"WFD\": {\"errs\": errs_wfd, \"cov\": C_wfd, \"labels\": labels},\n",
    "            \"WFD+Twilight\": {\"errs\": errs_tw, \"cov\": C_tw, \"labels\": labels}}\n",
    "\n",
    "# Colors\n",
    "COLORS = {\n",
    "    \"WFD\": {\"primary\": \"#1f77b4\", \"light\": \"#9ecae1\"},\n",
    "    \"WFD+Twilight\": {\"primary\": \"#2ca02c\", \"light\": \"#98df8a\"},\n",
    "}\n",
    "FID_CENTERS = {\"Om\": 0.3, \"w0\": -1.0, \"wa\": 0.0}\n",
    "\n",
    "def ellipse_points_from_cov(C2, n_sigma=1.0, n_pts=400):\n",
    "    delta = 2.30 if np.isclose(n_sigma,1.0) else (6.17 if np.isclose(n_sigma,2.0) else n_sigma**2)\n",
    "    vals, vecs = np.linalg.eigh(C2)\n",
    "    order = vals.argsort()[::-1]\n",
    "    vals = vals[order]; vecs = vecs[:, order]\n",
    "    a = np.sqrt(delta * vals[0]); b = np.sqrt(delta * vals[1])\n",
    "    t = np.linspace(0, 2*np.pi, n_pts)\n",
    "    circ = np.stack([a*np.cos(t), b*np.sin(t)], axis=0)\n",
    "    pts = (vecs @ circ)\n",
    "    return pts[0], pts[1]\n",
    "\n",
    "def plot_corner(res, params=(\"Om\",\"w0\",\"wa\"), title=\"w0waCDM Forecast\"):\n",
    "    labs = res[\"WFD\"][\"labels\"]; idx = [labs.index(p) for p in params]; n = len(params)\n",
    "    fig, axes = plt.subplots(n, n, figsize=(3.2*n, 3.2*n), dpi=150)\n",
    "    for r in range(n):\n",
    "        for c in range(n):\n",
    "            ax = axes[r, c]\n",
    "            if r < c: ax.axis('off'); continue\n",
    "            p_i = params[c]\n",
    "            if r == c:\n",
    "                for label in [\"WFD\", \"WFD+Twilight\"]:\n",
    "                    C = res[label][\"cov\"]; i = labs.index(p_i); s = np.sqrt(C[i,i])\n",
    "                    x0 = FID_CENTERS[p_i]; xs = np.linspace(x0-5*s, x0+5*s, 600)\n",
    "                    ys = np.exp(-0.5*((xs-x0)/s)**2)/(s*np.sqrt(2*np.pi))\n",
    "                    ax.plot(xs, ys, color=COLORS[label][\"primary\"], lw=2)\n",
    "                ax.set_ylabel(\"PDF\"); ax.set_xlabel(p_i)\n",
    "            else:\n",
    "                p_x, p_y = params[c], params[r]; i, j = labs.index(p_x), labs.index(p_y)\n",
    "                for label in [\"WFD\", \"WFD+Twilight\"]:\n",
    "                    C = res[label][\"cov\"]; C2 = C[np.ix_([i,j],[i,j])]\n",
    "                    ex, ey = ellipse_points_from_cov(C2, n_sigma=1.0)\n",
    "                    ex2, ey2 = ellipse_points_from_cov(C2, n_sigma=2.0)\n",
    "                    ax.plot(FID_CENTERS[p_x]+ex,  FID_CENTERS[p_y]+ey,  color=COLORS[label][\"primary\"], lw=2)\n",
    "                    ax.plot(FID_CENTERS[p_x]+ex2, FID_CENTERS[p_y]+ey2, color=COLORS[label][\"light\"],   lw=1.5)\n",
    "                ax.set_xlabel(p_x); ax.set_ylabel(p_y)\n",
    "    from matplotlib.lines import Line2D\n",
    "    handles = [Line2D([0],[0], color=COLORS[\"WFD\"][\"primary\"], lw=2, label=\"WFD 1σ\"),\n",
    "               Line2D([0],[0], color=COLORS[\"WFD\"][\"light\"],   lw=1.5, label=\"WFD 2σ\"),\n",
    "               Line2D([0],[0], color=COLORS[\"WFD+Twilight\"][\"primary\"], lw=2, label=\"WFD+Twilight 1σ\"),\n",
    "               Line2D([0],[0], color=COLORS[\"WFD+Twilight\"][\"light\"],   lw=1.5, label=\"WFD+Twilight 2σ\")]\n",
    "    axes[0,0].legend(handles=handles, frameon=False, loc=\"upper right\")\n",
    "    fig.suptitle(title, y=0.93); fig.tight_layout(); plt.show()\n",
    "\n",
    "def plot_lcdm_1d(res, param=\"Om\", title=\"ΛCDM Forecast\"):\n",
    "    fig = plt.figure(dpi=150); ax = plt.gca(); x0 = FID_CENTERS[param]\n",
    "    sigs = {}\n",
    "    for label in [\"WFD\", \"WFD+Twilight\"]:\n",
    "        labs = res[label][\"labels\"]; C = res[label][\"cov\"]; i = labs.index(param); sigs[label] = np.sqrt(C[i,i])\n",
    "    span = 5.0*max(sigs.values()); xs = np.linspace(x0-3*span, x0+3*span, 800)\n",
    "    for label in [\"WFD\", \"WFD+Twilight\"]:\n",
    "        s = sigs[label]; ys = np.exp(-0.5*((xs-x0)/s)**2)/(s*np.sqrt(2*np.pi))\n",
    "        ax.plot(xs, ys, label=f\"{label} (σ={s:.4g})\", color=COLORS[label][\"primary\"])\n",
    "    ax.set_xlabel(param); ax.set_ylabel(\"Gaussian (norm.)\"); ax.legend(frameon=False)\n",
    "    if title: plt.title(title); plt.show()\n",
    "\n",
    "# Run forecasts & plots\n",
    "res_lcdm  = run_binned_forecast(df_wfd_bin, df_twfull_bin, model=\"lcdm\")\n",
    "res_w0wa  = run_binned_forecast(df_wfd_bin, df_twfull_bin, model=\"w0wa\")\n",
    "\n",
    "def _print_errors(res, name):\n",
    "    labs = res[\"WFD\"][\"labels\"]\n",
    "    sig_wfd = {p: np.sqrt(res[\"WFD\"][\"cov\"][i,i]) for i,p in enumerate(labs)}\n",
    "    sig_tw  = {p: np.sqrt(res[\"WFD+Twilight\"][\"cov\"][i,i]) for i,p in enumerate(labs)}\n",
    "    print(f\"\\n[{name}] 1σ parameter uncertainties\")\n",
    "    for p in labs:\n",
    "        print(f\"  {p:>3s}:  WFD={sig_wfd[p]:.4g}   WFD+Twilight={sig_tw[p]:.4g}   improvement x{sig_wfd[p]/sig_tw[p]:.2f}\")\n",
    "\n",
    "_print_errors(res_lcdm, \"ΛCDM\")\n",
    "_print_errors(res_w0wa, \"w0waCDM\")\n",
    "\n",
    "plot_lcdm_1d(res_lcdm, param=\"Om\", title=\"ΛCDM: constraint on Ωm\")\n",
    "plot_corner(res_w0wa, params=(\"Om\",\"w0\",\"wa\"), title=\"w0waCDM: Ωm–w0–wa constraints (1σ/2σ)\")\n"
   ],
   "id": "a9e78d8588504080",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ===================================================================\n",
    "# START: Replacement and new code block\n",
    "# ===================================================================\n",
    "\n",
    "# --- Run forecasts for all three scenarios ---\n",
    "# Baseline vs. Twilight\n",
    "res_lcdm  = run_binned_forecast(df_wfd_bin, df_twfull_bin, model=\"lcdm\")\n",
    "res_w0wa  = run_binned_forecast(df_wfd_bin, df_twfull_bin, model=\"w0wa\")\n",
    "\n",
    "# Baseline vs. Your Guess\n",
    "res_guess_lcdm = run_binned_forecast(df_wfd_bin, df_twguess_bin, model=\"lcdm\")\n",
    "res_guess_w0wa = run_binned_forecast(df_wfd_bin, df_twguess_bin, model=\"w0wa\")\n",
    "\n",
    "\n",
    "# --- Print a comparison of all results ---\n",
    "def _print_errors_comparison(res_orig, res_guess, name):\n",
    "    labs = res_orig[\"WFD\"][\"labels\"]\n",
    "    sig_wfd = {p: res_orig[\"WFD\"][\"errs\"][p] for p in labs}\n",
    "    sig_tw = {p: res_orig[\"WFD+Twilight\"][\"errs\"][p] for p in labs}\n",
    "    sig_guess = {p: res_guess[\"WFD+Twilight\"][\"errs\"][p] for p in labs} # Note: uses res_guess\n",
    "\n",
    "    print(f\"\\n[{name}] 1-sigma parameter uncertainty comparison\")\n",
    "    for p in labs:\n",
    "        print(f\"  {p:>3s}:\")\n",
    "        print(f\"    WFD               = {sig_wfd[p]:.4g}\")\n",
    "        print(f\"    WFD+Twilight      = {sig_tw[p]:.4g}   (improvement x{sig_wfd[p]/sig_tw[p]:.2f})\")\n",
    "        print(f\"    WFD+Twilight(Guess)= {sig_guess[p]:.4g}   (improvement x{sig_wfd[p]/sig_guess[p]:.2f})\")\n",
    "\n",
    "_print_errors_comparison(res_lcdm, res_guess_lcdm, \"ΛCDM\")\n",
    "_print_errors_comparison(res_w0wa, res_guess_w0wa, \"w0waCDM\")\n",
    "\n",
    "\n",
    "# --- Update plotting functions to include the third scenario ---\n",
    "COLORS[\"WFD+Twilight (Guess)\"] = {\"primary\": \"#ff7f0e\", \"light\": \"#ffbb78\"} # Define a color for the new scenario\n",
    "\n",
    "def plot_lcdm_1d_comparison(res_list, param=\"Om\", title=\"ΛCDM Forecast Comparison\"):\n",
    "    fig = plt.figure(dpi=150); ax = plt.gca(); x0 = FID_CENTERS[param]\n",
    "    max_s = 0\n",
    "    for label, res in res_list.items():\n",
    "        labs = res[\"labels\"]; C = res[\"cov\"]; i = labs.index(param); s = np.sqrt(C[i,i])\n",
    "        max_s = max(max_s, s)\n",
    "\n",
    "    span = 5.0*max_s; xs = np.linspace(x0-3*span, x0+3*span, 800)\n",
    "\n",
    "    for label, res in res_list.items():\n",
    "        labs = res[\"labels\"]; C = res[\"cov\"]; i = labs.index(param); s = np.sqrt(C[i,i])\n",
    "        ys = np.exp(-0.5*((xs-x0)/s)**2)/(s*np.sqrt(2*np.pi))\n",
    "        ax.plot(xs, ys, label=f\"{label} (σ={s:.4g})\", color=COLORS[label][\"primary\"])\n",
    "\n",
    "    ax.set_xlabel(param); ax.set_ylabel(\"Gaussian (norm.)\"); ax.legend(frameon=False)\n",
    "    if title: plt.title(title); plt.show()\n",
    "\n",
    "def plot_corner_comparison(res_list, params=(\"Om\",\"w0\",\"wa\"), title=\"w0waCDM Forecast Comparison (1σ/2σ)\"):\n",
    "    labs_ref = res_list[\"WFD\"][\"labels\"]; idx = [labs_ref.index(p) for p in params]; n = len(params)\n",
    "    fig, axes = plt.subplots(n, n, figsize=(3.2*n, 3.2*n), dpi=150)\n",
    "    for r in range(n):\n",
    "        for c in range(n):\n",
    "            ax = axes[r, c]\n",
    "            if r < c: ax.axis('off'); continue\n",
    "            p_i = params[c]\n",
    "            if r == c:\n",
    "                for label, res in res_list.items():\n",
    "                    C = res[\"cov\"]; i = labs_ref.index(p_i); s = np.sqrt(C[i,i])\n",
    "                    x0 = FID_CENTERS[p_i]; xs = np.linspace(x0-5*s, x0+5*s, 600)\n",
    "                    ys = np.exp(-0.5*((xs-x0)/s)**2)/(s*np.sqrt(2*np.pi))\n",
    "                    ax.plot(xs, ys, color=COLORS[label][\"primary\"], lw=2)\n",
    "                ax.set_ylabel(\"PDF\"); ax.set_xlabel(p_i)\n",
    "            else:\n",
    "                p_x, p_y = params[c], params[r]; i, j = labs_ref.index(p_x), labs_ref.index(p_y)\n",
    "                for label, res in res_list.items():\n",
    "                    C = res[\"cov\"]; C2 = C[np.ix_([i,j],[i,j])]\n",
    "                    ex, ey = ellipse_points_from_cov(C2, n_sigma=1.0)\n",
    "                    ex2, ey2 = ellipse_points_from_cov(C2, n_sigma=2.0)\n",
    "                    ax.plot(FID_CENTERS[p_x]+ex,  FID_CENTERS[p_y]+ey,  color=COLORS[label][\"primary\"], lw=2, label=f\"{label} 1σ\")\n",
    "                    ax.plot(FID_CENTERS[p_x]+ex2, FID_CENTERS[p_y]+ey2, color=COLORS[label][\"light\"],   lw=1.5, label=f\"{label} 2σ\")\n",
    "                ax.set_xlabel(p_x); ax.set_ylabel(p_y)\n",
    "\n",
    "    from matplotlib.lines import Line2D\n",
    "    handles = [Line2D([0],[0], color=COLORS[label][\"primary\"], lw=2, label=f\"{label} 1σ\") for label in res_list]\n",
    "    axes[0,0].legend(handles=handles, frameon=False, loc=\"upper right\")\n",
    "    fig.suptitle(title, y=0.93); fig.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# --- Generate final plots including all three scenarios ---\n",
    "all_res_lcdm = {\n",
    "    \"WFD\": res_lcdm[\"WFD\"],\n",
    "    \"WFD+Twilight\": res_lcdm[\"WFD+Twilight\"],\n",
    "    \"WFD+Twilight (Guess)\": res_guess_lcdm[\"WFD+Twilight\"]\n",
    "}\n",
    "all_res_w0wa = {\n",
    "    \"WFD\": res_w0wa[\"WFD\"],\n",
    "    \"WFD+Twilight\": res_w0wa[\"WFD+Twilight\"],\n",
    "    \"WFD+Twilight (Guess)\": res_guess_w0wa[\"WFD+Twilight\"]\n",
    "}\n",
    "\n",
    "plot_lcdm_1d_comparison(all_res_lcdm, param=\"Om\", title=\"ΛCDM: constraint on Ωm (Comparison)\")\n",
    "plot_corner_comparison(all_res_w0wa, params=(\"Om\",\"w0\",\"wa\"), title=\"w0waCDM: Ωm–w0–wa constraints (Comparison)\")\n",
    "\n",
    "# ===================================================================\n",
    "# END: Replacement and new code block\n",
    "# ==================================================================="
   ],
   "id": "db70808e7715579f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
