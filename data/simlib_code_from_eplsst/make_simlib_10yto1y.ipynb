{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shift SIMLIB MJDs to 2023\n",
    "\n",
    "This cell shifts `PEAKMJD` and all `S:` observation MJDs to the 2023 window while preserving relative timing within each block. It keeps formatting consistent with the input file and reports basic validation stats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import hashlib\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------\n",
    "# config\n",
    "# -----------------------\n",
    "input_path  = Path(\"LOWZ_REDSHIFT_LT015_FROM_SIMDATA.SIMLIB\")\n",
    "output_path = Path(\"LOWZ_REDSHIFT_LT015_FROM_SIMDATA_SHIFTED_TO_2023.SIMLIB\")\n",
    "\n",
    "start_date_str = \"2022-12-31\"\n",
    "end_date_str   = \"2023-12-31\"\n",
    "\n",
    "# -----------------------\n",
    "# MJD <-> datetime\n",
    "# -----------------------\n",
    "MJD_EPOCH = dt.datetime(1858, 11, 17, 0, 0, 0)\n",
    "\n",
    "def mjd_to_datetime(mjd: float) -> dt.datetime:\n",
    "    return MJD_EPOCH + dt.timedelta(days=float(mjd))\n",
    "\n",
    "def datetime_to_mjd(d: dt.datetime) -> float:\n",
    "    return (d - MJD_EPOCH).total_seconds() / 86400.0\n",
    "\n",
    "def replace_year_safe(d: dt.datetime, year: int) -> dt.datetime:\n",
    "    \"\"\"Replace year while keeping month/day/time; handle Feb29 -> Feb28 (or nearest valid).\"\"\"\n",
    "    try:\n",
    "        return d.replace(year=year)\n",
    "    except ValueError:\n",
    "        if d.month == 2 and d.day == 29:\n",
    "            return d.replace(year=year, day=28)\n",
    "        day = d.day\n",
    "        while day > 28:\n",
    "            day -= 1\n",
    "            try:\n",
    "                return d.replace(year=year, day=day)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        raise\n",
    "\n",
    "# -----------------------\n",
    "# formatting helpers\n",
    "# -----------------------\n",
    "def decimals_in_token(tok: str) -> int:\n",
    "    return len(tok.split(\".\")[-1]) if \".\" in tok else 0\n",
    "\n",
    "def format_like(val: float, template: str) -> str:\n",
    "    \"\"\"Format float like template token: preserve decimals and width.\"\"\"\n",
    "    dec = decimals_in_token(template)\n",
    "    s = f\"{val:.{dec}f}\"\n",
    "    return s.rjust(len(template))\n",
    "\n",
    "def round_to(val: float, decs: int) -> float:\n",
    "    return float(f\"{val:.{decs}f}\")\n",
    "\n",
    "def ceil_to(val: float, decs: int) -> float:\n",
    "    step = 10 ** (-decs)\n",
    "    return math.ceil(val / step - 1e-12) * step\n",
    "\n",
    "def floor_to(val: float, decs: int) -> float:\n",
    "    step = 10 ** (-decs)\n",
    "    return math.floor(val / step + 1e-12) * step\n",
    "\n",
    "# -----------------------\n",
    "# regex\n",
    "# -----------------------\n",
    "peak_token_re = re.compile(r\"(PEAKMJD:\\\\s*)([0-9]+(?:\\.[0-9]*)?)\")\n",
    "s_prefix_re   = re.compile(r\"^(S:\\\\s*)([0-9]+(?:\\.[0-9]*)?)\")\n",
    "nobs_re       = re.compile(r\"NOBS:\\\\s*([0-9]+)\")\n",
    "\n",
    "def update_peak_line(line: str, new_peak: float) -> str:\n",
    "    m = peak_token_re.search(line)\n",
    "    if not m:\n",
    "        return line\n",
    "    templ = m.group(2)\n",
    "    newtok = format_like(new_peak, templ)\n",
    "    return line[:m.start(2)] + newtok + line[m.end(2):]\n",
    "\n",
    "def update_s_line(line: str, new_mjd: float) -> str:\n",
    "    m = s_prefix_re.match(line)\n",
    "    if not m:\n",
    "        return line\n",
    "    templ = m.group(2)\n",
    "    newtok = format_like(new_mjd, templ)\n",
    "    return m.group(1) + newtok + line[m.end(2):]\n",
    "\n",
    "# -----------------------\n",
    "# parse blocks\n",
    "# -----------------------\n",
    "def parse_blocks(lines):\n",
    "    \"\"\"Return prefix_lines (before first LIBID) and list of blocks (each block = list[str]).\"\"\"\n",
    "    prefix = []\n",
    "    blocks = []\n",
    "    cur = None\n",
    "    for line in lines:\n",
    "        if line.startswith(\"LIBID:\"):\n",
    "            if cur is not None:\n",
    "                blocks.append(cur)\n",
    "            cur = [line]\n",
    "        else:\n",
    "            if cur is None:\n",
    "                prefix.append(line)\n",
    "            else:\n",
    "                cur.append(line)\n",
    "    if cur is not None:\n",
    "        blocks.append(cur)\n",
    "    return prefix, blocks\n",
    "\n",
    "# -----------------------\n",
    "# main shifting\n",
    "# -----------------------\n",
    "start_dt = dt.datetime.fromisoformat(start_date_str)  # 00:00:00\n",
    "end_dt_excl = dt.datetime.fromisoformat(end_date_str) + dt.timedelta(days=1)  # exclusive\n",
    "start_mjd = datetime_to_mjd(start_dt)\n",
    "end_mjd_excl = datetime_to_mjd(end_dt_excl)\n",
    "\n",
    "target_year = int(end_date_str.split(\"-\")[0])  # 2023\n",
    "\n",
    "text = input_path.read_text()\n",
    "lines = text.splitlines(keepends=True)\n",
    "prefix_lines, blocks = parse_blocks(lines)\n",
    "\n",
    "# stats & checks\n",
    "clamp_low = clamp_high = keep_monthday = 0\n",
    "round_adjust = 0\n",
    "\n",
    "max_dt_err = 0.0\n",
    "bad_range = []\n",
    "bad_sorted = []\n",
    "bad_nobs = []\n",
    "\n",
    "out_blocks = []\n",
    "\n",
    "for blk in blocks:\n",
    "    # find PEAKMJD line/token\n",
    "    old_peak = None\n",
    "    peak_idx = None\n",
    "    peak_tok = None\n",
    "    for i, line in enumerate(blk):\n",
    "        m = peak_token_re.search(line)\n",
    "        if m:\n",
    "            old_peak = float(m.group(2))\n",
    "            peak_idx = i\n",
    "            peak_tok = m.group(2)\n",
    "            break\n",
    "    if old_peak is None:\n",
    "        out_blocks.append(blk)\n",
    "        continue\n",
    "\n",
    "    peak_decs = decimals_in_token(peak_tok)\n",
    "\n",
    "    # collect S lines\n",
    "    s_idxs = []\n",
    "    mjds = []\n",
    "    mjd_toks = []\n",
    "    for i, line in enumerate(blk):\n",
    "        m = s_prefix_re.match(line)\n",
    "        if m:\n",
    "            s_idxs.append(i)\n",
    "            mjd_toks.append(m.group(2))\n",
    "            mjds.append(float(m.group(2)))\n",
    "\n",
    "    if not mjds:\n",
    "        out_blocks.append(blk)\n",
    "        continue\n",
    "\n",
    "    # relative times to peak (keep unchanged)\n",
    "    dts = [m - old_peak for m in mjds]\n",
    "    dt_min, dt_max = min(dts), max(dts)\n",
    "\n",
    "    # allowed new peak range so that all obs are within [start, end_excl)\n",
    "    allowed_min = start_mjd - dt_min\n",
    "    allowed_max = (end_mjd_excl - 1e-9) - dt_max\n",
    "\n",
    "    # candidate new peak: same month/day/time but in target_year\n",
    "    cand_dt = replace_year_safe(mjd_to_datetime(old_peak), target_year)\n",
    "    cand_mjd = datetime_to_mjd(cand_dt)\n",
    "\n",
    "    if cand_mjd < allowed_min:\n",
    "        new_peak_raw = allowed_min\n",
    "        clamp_low += 1\n",
    "        method = \"clamp_low\"\n",
    "    elif cand_mjd > allowed_max:\n",
    "        new_peak_raw = allowed_max\n",
    "        clamp_high += 1\n",
    "        method = \"clamp_high\"\n",
    "    else:\n",
    "        new_peak_raw = cand_mjd\n",
    "        keep_monthday += 1\n",
    "        method = \"monthday_keep\"\n",
    "\n",
    "    # quantize peak to file precision, then ensure still within allowed range\n",
    "    new_peak = round_to(new_peak_raw, peak_decs)\n",
    "    if new_peak < allowed_min - 1e-12:\n",
    "        new_peak = ceil_to(allowed_min, peak_decs)\n",
    "        round_adjust += 1\n",
    "    if new_peak > allowed_max + 1e-12:\n",
    "        new_peak = floor_to(allowed_max, peak_decs)\n",
    "        round_adjust += 1\n",
    "\n",
    "    # update PEAKMJD line\n",
    "    blk2 = list(blk)\n",
    "    blk2[peak_idx] = update_peak_line(blk2[peak_idx], new_peak)\n",
    "\n",
    "    # update S lines\n",
    "    for (idx, old_mjd, tok) in zip(s_idxs, mjds, mjd_toks):\n",
    "        mjd_decs = decimals_in_token(tok)\n",
    "        dt_old = old_mjd - old_peak\n",
    "        new_mjd = round_to(new_peak + dt_old, mjd_decs)\n",
    "        blk2[idx] = update_s_line(blk2[idx], new_mjd)\n",
    "\n",
    "        # range check\n",
    "        if not (start_mjd - 1e-6 <= new_mjd < end_mjd_excl + 1e-6):\n",
    "            bad_range.append((\"MJD\", blk2[0].strip(), new_mjd))\n",
    "\n",
    "        # dt check (file-level)\n",
    "        dt_new = new_mjd - new_peak\n",
    "        max_dt_err = max(max_dt_err, abs(dt_new - dt_old))\n",
    "\n",
    "    # sorted check\n",
    "    new_mjds = [float(s_prefix_re.match(blk2[i]).group(2)) for i in s_idxs]\n",
    "    if any(new_mjds[i] > new_mjds[i+1] + 1e-9 for i in range(len(new_mjds)-1)):\n",
    "        bad_sorted.append(blk2[0].strip())\n",
    "\n",
    "    # NOBS consistency check\n",
    "    nobs_decl = None\n",
    "    for line in blk2:\n",
    "        m = nobs_re.search(line)\n",
    "        if m and nobs_decl is None:\n",
    "            nobs_decl = int(m.group(1))\n",
    "            break\n",
    "    if nobs_decl is not None and nobs_decl != len(s_idxs):\n",
    "        bad_nobs.append((blk2[0].strip(), nobs_decl, len(s_idxs)))\n",
    "\n",
    "    # PEAK range check\n",
    "    if not (start_mjd - 1e-6 <= new_peak < end_mjd_excl + 1e-6):\n",
    "        bad_range.append((\"PEAK\", blk2[0].strip(), new_peak))\n",
    "\n",
    "    out_blocks.append(blk2)\n",
    "\n",
    "# write output\n",
    "out_text = \"\".join(prefix_lines) + \"\".join(\"\".join(b) for b in out_blocks)\n",
    "output_path.write_text(out_text)\n",
    "\n",
    "# checksums\n",
    "def md5(path: Path) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "print(\"=== DONE ===\")\n",
    "print(f\"Input : {input_path}\")\n",
    "print(f\"Output: {output_path}\")\n",
    "print()\n",
    "print(f\"Time window: [{start_date_str} 00:00:00, {end_dt_excl.date()} 00:00:00)  (MJD [{start_mjd:.1f}, {end_mjd_excl:.1f}))\")\n",
    "print(f\"Blocks total: {len(blocks)}\")\n",
    "print(f\"Peak mapping: keep_monthday={keep_monthday}, clamp_low={clamp_low}, clamp_high={clamp_high}, round_adjust={round_adjust}\")\n",
    "print()\n",
    "print(f\"max |(MJD-PEAK)_new - (MJD-PEAK)_old| = {max_dt_err:.3e} days\")\n",
    "print(f\"bad_range  = {len(bad_range)}\")\n",
    "print(f\"bad_sorted = {len(bad_sorted)}\")\n",
    "print(f\"bad_nobs   = {len(bad_nobs)}\")\n",
    "print()\n",
    "print(f\"MD5(input) = {md5(input_path)}\")\n",
    "print(f\"MD5(output)= {md5(output_path)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
